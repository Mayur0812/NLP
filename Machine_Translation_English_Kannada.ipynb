{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Translation_English-Kannada.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdr8li8UoeIu",
        "colab_type": "code",
        "outputId": "546dac64-b72b-42c9-892a-47681c8a96a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJ0dGvrpjO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8ac79f9-163c-4ceb-a10f-5a283f9a284e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import csv\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Embedding , LSTM , Dense\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping as early, ModelCheckpoint as checkpoint  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDncLg0ZpEOd",
        "colab_type": "code",
        "outputId": "8dd4a408-4d9b-493b-aba2-a4a680c685d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JAS-J_lp9WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('drive/My Drive/Colab Notebooks/Machine_Translation/kan-eng/kan.txt', sep=\"\\t\", header=None)\n",
        "data.columns = [\"eng\",\"kan\",\"created_by\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvQwEmMRrRVZ",
        "colab_type": "code",
        "outputId": "ee58ed41-c49f-4eb1-ef92-301b055a3c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>kan</th>\n",
              "      <th>created_by</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tom woke up.</td>\n",
              "      <td>ಟಾಮ್ ಏಳಿದನು.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Give me half.</td>\n",
              "      <td>ಅರ್ಧ ನನಗೆ ಕೊಡು.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We needed it.</td>\n",
              "      <td>ನಮಗೆ ಬೇಕಾಗಿತ್ತು.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tom liked you.</td>\n",
              "      <td>ಟಾಮ್ ನಿನ್ನನ್ನು ಬಯಸಿದನು.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just go inside.</td>\n",
              "      <td>ಒಳಗೆ ಹೋಗು ಅಷ್ಟೇ.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               eng  ...                                         created_by\n",
              "0     Tom woke up.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "1    Give me half.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "2    We needed it.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "3   Tom liked you.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "4  Just go inside.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toMNppEgpZMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(df):\n",
        "  \"\"\"\n",
        "  Preprocessing function to convert data into a standard form.\n",
        "  Since dataset is pretty much in standard format, little pre-processing is required.\n",
        "  Will also add start and end token to target data\n",
        "  <SOS> : will imply start of the sentence\n",
        "  <EOS> : will imply end of the sentence\n",
        "  \"\"\"\n",
        "  df[\"eng\"] = df[\"eng\"].apply(lambda x: x.lower()) # Converting to lower case\n",
        "  special_ch = set(string.punctuation) #Removing all special characters, if any\n",
        "  df[\"eng\"] = df[\"eng\"].apply(lambda x: ''.join(ch for ch in x if ch not in special_ch))\n",
        "  df[\"kan\"] = df[\"kan\"].apply(lambda x: x.replace('.',''))\n",
        "  remove_digits = str.maketrans('', '', digits)\n",
        "  df[\"eng\"] = df[\"eng\"].apply(lambda x: x.translate(remove_digits))\n",
        "  df[\"kan\"] = df[\"kan\"].apply(lambda x : '<SOS> '+ x + ' <EOS>')\n",
        "  \n",
        "  return df\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzZIMH8Ruwn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = preprocess(data).copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpW5rRg1u4vp",
        "colab_type": "code",
        "outputId": "b6dccbdf-4f96-44ea-b8b3-d4b517579ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>kan</th>\n",
              "      <th>created_by</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tom woke up</td>\n",
              "      <td>&lt;SOS&gt; ಟಾಮ್ ಏಳಿದನು &lt;EOS&gt;</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>give me half</td>\n",
              "      <td>&lt;SOS&gt; ಅರ್ಧ ನನಗೆ ಕೊಡು &lt;EOS&gt;</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>we needed it</td>\n",
              "      <td>&lt;SOS&gt; ನಮಗೆ ಬೇಕಾಗಿತ್ತು &lt;EOS&gt;</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tom liked you</td>\n",
              "      <td>&lt;SOS&gt; ಟಾಮ್ ನಿನ್ನನ್ನು ಬಯಸಿದನು &lt;EOS&gt;</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>just go inside</td>\n",
              "      <td>&lt;SOS&gt; ಒಳಗೆ ಹೋಗು ಅಷ್ಟೇ &lt;EOS&gt;</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              eng  ...                                         created_by\n",
              "0     tom woke up  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "1    give me half  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "2    we needed it  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "3   tom liked you  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "4  just go inside  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koElr2RcvCc5",
        "colab_type": "code",
        "outputId": "c3521afe-40a1-4c86-affa-45ae82f7ab6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TugCN-lb2zT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_vocab = set()\n",
        "for line in data['eng']:\n",
        "  for word in line.split():\n",
        "    if word not in eng_vocab:\n",
        "      eng_vocab.add(word)\n",
        "\n",
        "kan_vocab=set()\n",
        "for kan in data.kan:\n",
        "    for word in kan.split():\n",
        "        if word not in kan_vocab:\n",
        "            kan_vocab.add(word)\n",
        "\n",
        "### Identifying maximum length of sequences in source and destination:\n",
        "\n",
        "len_eng = []\n",
        "for line in data['eng']:\n",
        "  len_eng.append(len(line.split(' ')))\n",
        "\n",
        "max_len_src = np.max(len_eng)\n",
        "\n",
        "len_kan = []\n",
        "for line in data['kan']:\n",
        "  len_kan.append(len(line.split(' ')))\n",
        "\n",
        "max_len_tar = np.max(len_kan)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7FAd3CdKHO9",
        "colab_type": "code",
        "outputId": "d3782c28-dabe-404d-de78-1f5960315e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Maximum length of sentence in source language: \",max_len_src )\n",
        "print(\"Maximum length of sentence in source language: \",max_len_tar )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum length of sentence in source language:  15\n",
            "Maximum length of sentence in source language:  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Bu14d0KI-T",
        "colab_type": "code",
        "outputId": "a9dacd43-c399-4c7e-9138-217acdbda45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "input_words = sorted(list(eng_vocab))\n",
        "target_words = sorted(list(kan_vocab))\n",
        "print(\"Input words: \\n\", input_words)\n",
        "print(\"Output Words: \\n\", target_words)\n",
        "num_encoder_tokens = len(eng_vocab)\n",
        "num_decoder_tokens = len(kan_vocab)\n",
        "num_decoder_tokens += 1 \n",
        "\n",
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
        "\n",
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input words: \n",
            " ['a', 'about', 'abroad', 'accident', 'across', 'afraid', 'after', 'ago', 'agreement', 'agrees', 'all', 'almost', 'always', 'among', 'an', 'and', 'animals', 'announced', 'another', 'any', 'anybody', 'anything', 'apology', 'are', 'argentina', 'arrived', 'as', 'ask', 'at', 'awake', 'away', 'baby', 'back', 'be', 'beach', 'because', 'been', 'before', 'believe', 'believed', 'better', 'book', 'bored', 'borrow', 'both', 'bottles', 'bought', 'brazil', 'brother', 'brought', 'bucket', 'but', 'buy', 'by', 'call', 'came', 'can', 'cant', 'car', 'careless', 'caught', 'cause', 'caused', 'cellar', 'charge', 'children', 'coffee', 'competitor', 'concert', 'conditions', 'cost', 'could', 'couldnt', 'couple', 'crowd', 'crush', 'dangerous', 'daughter', 'day', 'deal', 'death', 'decided', 'declared', 'did', 'didnt', 'difficult', 'do', 'does', 'doesnt', 'doing', 'dollars', 'dont', 'down', 'dress', 'drink', 'during', 'earlier', 'eat', 'eaten', 'else', 'engagement', 'enough', 'even', 'ever', 'every', 'everything', 'exactly', 'exchanged', 'exciting', 'excuse', 'expected', 'factory', 'failed', 'family', 'father', 'fathers', 'fault', 'feel', 'few', 'find', 'finished', 'fire', 'fired', 'firefighters', 'firm', 'first', 'fish', 'five', 'food', 'for', 'forget', 'forgot', 'forgotten', 'french', 'from', 'fulfill', 'gave', 'get', 'getting', 'give', 'glad', 'glasses', 'go', 'goes', 'going', 'gone', 'good', 'gorgeous', 'got', 'gotten', 'greasy', 'great', 'greet', 'guests', 'guitar', 'guys', 'gym', 'had', 'hadnt', 'half', 'hand', 'happened', 'has', 'have', 'he', 'help', 'her', 'here', 'him', 'his', 'home', 'honest', 'hope', 'horrible', 'hours', 'how', 'hug', 'huge', 'hurt', 'i', 'id', 'if', 'ill', 'im', 'in', 'injuries', 'inside', 'interesting', 'is', 'isnt', 'it', 'its', 'ive', 'jam', 'job', 'just', 'keep', 'killer', 'killing', 'knew', 'know', 'last', 'lawyer', 'leave', 'lecture', 'left', 'let', 'lets', 'like', 'liked', 'likely', 'longer', 'looks', 'lost', 'lot', 'love', 'loved', 'lucky', 'made', 'make', 'man', 'many', 'married', 'mary', 'may', 'maybe', 'me', 'means', 'meat', 'met', 'mind', 'mine', 'minor', 'missed', 'money', 'more', 'morning', 'moving', 'much', 'museum', 'must', 'my', 'myself', 'name', 'need', 'needed', 'never', 'new', 'night', 'nine', 'no', 'not', 'nothing', 'now', 'of', 'off', 'office', 'on', 'once', 'one', 'only', 'open', 'opinion', 'order', 'our', 'out', 'over', 'owe', 'page', 'paper', 'paris', 'people', 'photograph', 'pigsty', 'places', 'play', 'posed', 'possibly', 'pounds', 'programs', 'promised', 'put', 'raw', 'read', 'reading', 'ready', 'real', 'really', 'recommend', 'regards', 'relocated', 'remember', 'return', 'right', 'river', 'room', 'sadly', 'same', 'say', 'scar', 'search', 'see', 'she', 'should', 'shouldnt', 'shouldve', 'sick', 'sight', 'since', 'solution', 'some', 'something', 'sorry', 'spoke', 'spring', 'stared', 'station', 'stay', 'stayed', 'still', 'storm', 'story', 'stupidity', 'suffered', 'suggest', 'sure', 'swam', 'talk', 'taught', 'team', 'teens', 'tell', 'temper', 'thank', 'thanking', 'that', 'thats', 'the', 'their', 'there', 'theres', 'these', 'they', 'theyre', 'thing', 'think', 'this', 'thought', 'three', 'time', 'to', 'told', 'tom', 'toms', 'too', 'took', 'traffic', 'tragedy', 'traveled', 'treats', 'tried', 'trip', 'truth', 'try', 'twice', 'two', 'unknown', 'up', 'us', 'useless', 'usually', 'vegetables', 'very', 'want', 'wanted', 'war', 'was', 'watch', 'watching', 'way', 'we', 'week', 'well', 'went', 'were', 'weve', 'what', 'whatever', 'whats', 'when', 'where', 'who', 'why', 'will', 'window', 'wine', 'wish', 'with', 'woke', 'wondered', 'wont', 'work', 'working', 'would', 'wouldve', 'wrong', 'year', 'you', 'your', 'zealand']\n",
            "Output Words: \n",
            " ['(೨೩೦', '<EOS>', '<SOS>', 'ಅಂತ', 'ಅಂದಿನಿಂದ', 'ಅಗ್ನಿಶಾಮಕರು', 'ಅಡ್ಡಲಾಗಿ', 'ಅಣ್ಣ', 'ಅತಿಥಿಗಳನ್ನು', 'ಅದಕ್ಕಿಂತ', 'ಅದಕ್ಕೆ', 'ಅದಕ್ಕೇನೇ', 'ಅದನ್ನು', 'ಅದನ್ನೇ', 'ಅದರ', 'ಅದಷ್ಟೇ', 'ಅದು', 'ಅದೃಷ್ಟ', 'ಅದೇ', 'ಅದ್ಭುತವಾಯಿತು', 'ಅಧಿಕಾರ', 'ಅನಿಸಿತು', 'ಅನಿಸುತ್ತಾ?', 'ಅನಿಸುತ್ತೆ', 'ಅಪಘಾತದ', 'ಅಪಘಾತವಾಯಿತು', 'ಅಪಾಯ', 'ಅಭಿಪ್ರಾಯವನ್ನು', 'ಅರ್ಜೆಂಟೀನದ', 'ಅರ್ಥ', 'ಅರ್ಧ', 'ಅಲ್ಲವ?', 'ಅಲ್ಲೇ', 'ಅಲ್ವ?', 'ಅವನ', 'ಅವನನ್ನು', 'ಅವನಿಗೆ', 'ಅವನು', 'ಅವರ', 'ಅವರು', 'ಅವಳ', 'ಅವಳನ್ನು', 'ಅವಳಿಗೆ', 'ಅವಳು', 'ಅಷ್ಟೇ', 'ಅಸಡ್ಡೆ', 'ಆ', 'ಆಕರ್ಷಕವಾಗಿದ್ದರೆ', 'ಆಗಲಿಲ್ಲ', 'ಆಗಲ್ಲ', 'ಆಗುವುದಕ್ಕೆ', 'ಆಗುವುದಿಲ್ಲವೆಂದು', 'ಆಟ', 'ಆಡುತ್ತೀರಾ?', 'ಆದರೆ', 'ಆಫೀಸಿನಿಂದ', 'ಆಯಿತು', 'ಆಶಿಸುತ್ತೇನೆ', 'ಆಹಾರಕ್ಕೆ', 'ಇಟ್ಟಿದ್ದೇನೆ', 'ಇಟ್ಟುಕೊಂಡಿದ್ದಾಳೆ', 'ಇಡತ್ತೆ', 'ಇಡು', 'ಇತ್ತು', 'ಇದನ್ನು', 'ಇದು', 'ಇದೆ', 'ಇದ್ದವೋ', 'ಇದ್ದೇವೆ', 'ಇದ್ಯಾ?', 'ಇನ್ನಷ್ಟು', 'ಇನ್ನು', 'ಇನ್ನೂ', 'ಇನ್ನೊಂದು', 'ಇಬ್ಬರು', 'ಇರಬಾರದು', 'ಇರಬೇಕು', 'ಇರಲಿಲ್ಲ', 'ಇರುವುದಕ್ಕೆ', 'ಇಲ್ಲ', 'ಇಲ್ಲವೆಂದರೆ', 'ಇಲ್ಲವೇ', 'ಇಲ್ಲಿ', 'ಇವತ್ತು', 'ಇವೆ', 'ಇಷ್ಟಪಡುತ್ತಿದೆ', 'ಇಷ್ಟಪಡುತ್ತೇನೆ', 'ಈ', 'ಈಗ', 'ಈಜಿದನು', 'ಉಪನಾಮ', 'ಉಪಯೋಗವಿಲ್ಲ', 'ಉಳಿದುಕೊಂಡನು', 'ಎಂದಿಗೂ', 'ಎಂದು', 'ಎಚ್ಚರವಾಗಿರಬೇಕು', 'ಎಚ್ಚರವಾಗಿರಲು', 'ಎಣ್ಣೆ', 'ಎರಡರಷ್ಟು', 'ಎರಡು', 'ಎಲ್ಲದರ', 'ಎಲ್ಲಿ', 'ಎಲ್ಲಿದೆ?', 'ಎಳೆಮಗು', 'ಎಷ್ಟು', 'ಏನಕ್ಕೂ', 'ಏನನ್ನು', 'ಏನಾಯಿತು', 'ಏನು', 'ಏನೂ', 'ಏನೆಲ್ಲಾ', 'ಏನೋ', 'ಏಳಬೇಕಾಗಿತ್ತು', 'ಏಳಿದನು', 'ಐದು', 'ಒಂಟಿಯಾಗಿ', 'ಒಂದು', 'ಒಂದೇ', 'ಒಂಬತ್ತನೆಯ', 'ಒಪ್ಪ೦ದವನ್ನು', 'ಒಬ್ಬ', 'ಒಬ್ಬರು', 'ಒಳಗಡೆ', 'ಒಳಗೆ', 'ಒಳ್ಳೆಯ', 'ಓದಿ', 'ಓದಿದೆ', 'ಕಂಡಿದರು', 'ಕಂಡುಹಿಡಿಯಬಹುದು?', 'ಕಚ್ಚಾ', 'ಕಛೇರಿಗೆ', 'ಕಥೆ', 'ಕನ್ನಡಕ,', 'ಕರಣ', 'ಕಳೆದುಕೊಂಡ', 'ಕಷ್ಟವಿದೆ', 'ಕಾಫಿ', 'ಕಾರಣದಿಂದ', 'ಕಾರಣವೇ', 'ಕಾರನ್ನು', 'ಕಾರ್ಖಾನೆಯಲ್ಲಿ', 'ಕಾರ್ಯಕ್ರಮಗಳು', 'ಕಿಟಕಿಯ', 'ಕುಟುಂಬಕ್ಕೆ', 'ಕುಡಿಯಬೇಕು', 'ಕುರಿತು', 'ಕೆಲವೊಂದು', 'ಕೆಲಸ', 'ಕೆಲಸದಲ್ಲಿ', 'ಕೆಲಸದಿಂದ', 'ಕೊಠಡಿ', 'ಕೊಲೆಗಾರ', 'ಕೇಳಿ', 'ಕೇಳಿದೆ', 'ಕೈಯಲ್ಲಿ', 'ಕೊಟ್ಟನು', 'ಕೊಡಲಿಲ್ಲ', 'ಕೊಡು', 'ಕೊಲ್ಲಲ್ಲ', 'ಕೋಪ', 'ಕೋರಬೇಕು', 'ಕ್ಷಮಿಸಿ', 'ಕ್ಷಮೆ', 'ಕ್ಷಮೆಯು', 'ಖಚಿತಪಡಿಸಿಕೊಂಡೆ', 'ಖಚಿತವಾಗಿ', 'ಖರೀದಿಸಿದನು', 'ಖರೀದಿಸುವುದಕ್ಕೆ', 'ಗಂಟೆಗೆ', 'ಗಂಟೆಯ', 'ಗಾಯ', 'ಗಿಟಾರ್', 'ಗೆ)', 'ಗೊತ್ತಿಲ್ಲ', 'ಗೊತ್ತ?', 'ಗೊತ್ತಾಗುತ್ತಾ', 'ಗೊತ್ತಿತ್ತು', 'ಗೊತ್ತಿಲ್ಲ', 'ಗೊತ್ತೇ', 'ಗೊತ್ತೋ', 'ಘಟನೆಗಳು', 'ಚನ್ನಾಗಿ', 'ಜನಸಮೊಹದ', 'ಜರಗಿಸುತ್ತೀರಾ?', 'ಜಾಗದಲ್ಲಿ', 'ಜಾಸ್ತಿ', 'ಜಿಮ್ಮಿಗೆ', 'ಜಿಲಂಡಿಗೆ', 'ಜೊತೆ', 'ಜೋಡಿ', 'ಟಮಿನ', 'ಟಾಮಿಗೆ', 'ಟಾಮಿನ', 'ಟಾಮ್', 'ಟೊಮನ್ನು', 'ಠಾಣೆಗೆ', 'ಡಾಲರ್ಗಳನ್ನು', 'ತಂಡಕ್ಕೆ', 'ತಂದೆ', 'ತಂದೆಯ', 'ತಡೆಯುವುದಕ್ಕೆ', 'ತಪ್ಪಿಸಿಕೊಳ್ಳಬಹುದಾಗಿತ್ತು', 'ತಪ್ಪು', 'ತಬ್ಬಿಕೊಂಡೆ', 'ತಯಾರಾಗಿದ್ದಾನೆ', 'ತರಕಾರಿಗಳು', 'ತರಹ', 'ತಲುಪಿದ', 'ತಾನೇ', 'ತಿಂದಿದ್ದೀರಾ?', 'ತಿನ್ನು', 'ತಿನ್ನುವುದಿಲ್ಲ', 'ತಿಳಿದು', 'ತಿಳಿಸುತ್ತೇನೆ', 'ತುಂಬಾ', 'ತುಂಬಾನೇ', 'ತೆಗೆದಿರುವುದಕ್ಕೆ', 'ತೆಗೆದುಕೊಂಡು', 'ತೆರೆಯಿರಿ', 'ತೊಂದರೆ', 'ದಡಕ್ಕೆ', 'ದುಃಖದಿಂದ', 'ದುಡ್ಡನ್ನು', 'ದುಡ್ಡು', 'ದುರಂತ', 'ದೂರ', 'ಧನ್ಯವಾದ', 'ನಂದಿಸುವುದಕ್ಕೆ', 'ನಂದೇ', 'ನಂಬಿಕೆ', 'ನಂಬಿದ್ದೀಯಾ', 'ನಂಬಿದ್ದೇನೆ', 'ನಂಬೋಣ', 'ನಡಿತಾಯಿದೆ?', 'ನಡುವೆ', 'ನಡೆದಿವೆ', 'ನಡೆಯಿತು', 'ನಡೆಯಿತು?', 'ನಡೆಯುತ್ತಿದೆ', 'ನಡೆಯುತ್ತಿರುವಾಗ', 'ನದಿ', 'ನನಗೆ', 'ನನಗೇನೆ', 'ನನ್ನ', 'ನನ್ನದಿಲ್ಲ', 'ನನ್ನನು', 'ನನ್ನನ್ನು', 'ನನ್ನನ್ನೇ', 'ನಮಗೆ', 'ನಮ್ಮ', 'ನಮ್ಮಲ್ಲಿ', 'ನಾನು', 'ನಾನೇ', 'ನಾವು', 'ನಿಜವಾಗಿಯೂ', 'ನಿನಗೆ', 'ನಿನನ್ನು', 'ನಿನ್ನ', 'ನಿನ್ನನ್ನು', 'ನಿಮಗೆ', 'ನಿಮ್ಮ', 'ನಿಮ್ಮನ್ನು', 'ನಿಯಮಗಳನ್ನು', 'ನಿರಾಶೆ', 'ನಿರೀಕ್ಷಿಸುತ್ತಾ', 'ನಿರ್ದರಿಸಿದ', 'ನಿಲ್ಲಿದರು', 'ನಿಶ್ಚಿತಾರ್ಥದವನ್ನು', 'ನೀಡಿ', 'ನೀಡುತ್ತಾನೆಂದು', 'ನೀನು', 'ನೀವು', 'ನೀವೆಲ್ಲ', 'ನೆನಪು', 'ನೆನ್ನೆ', 'ನೆಲಮಾಳಿಗೆಗೆ', 'ನೋಡುತ್ತಲಿರುತ್ತಿದ್ದನು', 'ನೇರವೇರಿಸುವುದಕ್ಕೆ', 'ನೋಡುತ್ತಿರಲಿಲ್ಲವೆಂದು', 'ನೋಡುತ್ತೇನೆ', 'ನೋಡುವುದಕ್ಕೆ', 'ನೋಯಿಸಿದ್ದಾನಾ?', 'ನ್ಯೂ', 'ಪಟ್ಟರು', 'ಪತ್ರಿಕೆಯನ್ನು', 'ಪಯಣ', 'ಪರವಾಗಿ', 'ಪರಿಗಣಿಸುತ್ತಾಳೆ', 'ಪರಿಹಾರ', 'ಪರಿಹಾರವನ್ನು', 'ಪುಟಕ್ಕೆ', 'ಪುಸ್ತಕ', 'ಪುಸ್ತಕವನ್ನು', 'ಪೌಂಡ್ಸ್ಗೆ', 'ಪ್ಯಾರಿಸಿಗೆ', 'ಪ್ರಕಟಿಸಿದರು', 'ಪ್ರಕಟಿಸಿದ್ದಾರೆ', 'ಪ್ರತಿದಿನ', 'ಪ್ರತಿವರ್ಷ', 'ಪ್ರತಿಸ್ಪರ್ಧಿ', 'ಪ್ರಯತ್ನಿಸದೇ', 'ಪ್ರಯತ್ನಿಸಿದಳು', 'ಪ್ರವಚನ', 'ಪ್ರಶ್ನೆ', 'ಪ್ರಾಣಿಗಳು', 'ಪ್ರಾಮಾಣಿಕವಾಗಿ', 'ಪ್ರೀತಿ', 'ಪ್ರೀತಿಸುತ್ತಿದ್ದೀಯ', 'ಫೋಟೋಗೆ', 'ಫ್ರೆಂಚ್', 'ಬಂದಿದ್ದು', 'ಬಂದೆ', 'ಬಗ್ಗೆ', 'ಬಟ್ಟೆಯನ್ನು', 'ಬದಲಾಯಿಸಬಹುದು?', 'ಬನ್ನಿ', 'ಬಯಸಿದನು', 'ಬರಬೇಕಾಗಿತ್ತು', 'ಬರವುದಕ್ಕೆ', 'ಬರಿ', 'ಬರುತ್ತದೆ', 'ಬರುವ', 'ಬರುವದಕ್ಕೆ', 'ಬಹಳ', 'ಬಹಳಷ್ಟು', 'ಬಹುಮಟ್ಟಿಗೆ', 'ಬಹುಷಃ', 'ಬಾಟಲಿ', 'ಬಾದಲಿಯಲ್ಲಿ', 'ಬಾರಿ', 'ಬಾರಿಗೆ', 'ಬಿಗಿಯಾಗಿ', 'ಬಿಡಬೇಕಾ?', 'ಬಿಡಬೇಡಿ', 'ಬಿರುಗಾಳಿಯಿಂದ', 'ಬೆಂಕಿಗೆ', 'ಬೆಂಕಿಯನ್ನು', 'ಬೆಲೆ', 'ಬೆಳಿಗ್ಗೆ', 'ಬೇಕಾಗಿತ್ತು', 'ಬೇಕಾಗಿದ್ದು', 'ಬೇಕಾಗುತ್ತದೆ', 'ಬೇಕಾದರೆ', 'ಬೇಕಿತ್ತೋ', 'ಬೇಕೆಂದರೆ,', 'ಬೇಗ', 'ಬೇಡ', 'ಬೇರೆ', 'ಬೇರೆಯಾರನ್ನೂ', 'ಬೇಸರವಾಗಿತ್ತು', 'ಬ್ರಸಿಲು', 'ಭಯಾನಕವಾದ', 'ಭರವಸೆ', 'ಭಾರಿಸುವುದಕ್ಕೆ', 'ಭಾವಿಸುವುದಿಲ್ಲ', 'ಭೇಟಿಯಾಗಿಲ್ಲವೆಂದು', 'ಮಕ್ಕಳನ್ನು', 'ಮಗಳಿಗೆ', 'ಮತ್ತು', 'ಮದುವೆಯಾಗಿದಳು', 'ಮನಗೆ', 'ಮನೆಗೆ', 'ಮರಣಾನಂತರ', 'ಮರೆತುಬಿಟ್ಟಿದ್ದನು', 'ಮರೆತುಹೋಯಿತು', 'ಮರೆಯಬೇಡಿ', 'ಮಾಂಸ', 'ಮಾಡಕ್ಕೆ', 'ಮಾಡಬೇಕೆಂದು', 'ಮಾಡಬೇಡವೆಂದು', 'ಮಾಡಿದೆ', 'ಮಾಡುತ್ತಾ', 'ಮಾಡುತ್ತಾರೆ', 'ಮಾಡುತ್ತೀಯ', 'ಮಾಡುವುದಕ್ಕೆ', 'ಮಾಡೋಣ', 'ಮಾಡ್ಬೇಕು', 'ಮಾತನಾಡುತ್ತೇಯ', 'ಮಾತಾಡುತ್ತಾನೆ', 'ಮಾತ್ರ', 'ಮೀನು', 'ಮುಂಚೆ', 'ಮುಂಚೇನೇ', 'ಮುಂದೆ', 'ಮುಗಿಸಿದ್ದೀರಾ?', 'ಮೂರು', 'ಮೂರ್ಖತನಕ್ಕೆ', 'ಮೇರಿ', 'ಮೇರಿಗೆ', 'ಮೇಲೆ', 'ಮೊದಲ', 'ಯಾಕೆ', 'ಯಾರು', 'ಯಾರೂ', 'ಯಾವ', 'ಯಾವಾಗ', 'ಯಾವಾಗಲಾದರು', 'ಯಾವಾಗಲಾದರೂ', 'ಯುದ್ಧ', 'ಯುದ್ಧವನ್ನು', 'ಯೋಚನೆ', 'ರದ್ಧುಗೊಳಿಸಬೇಕು', 'ರಾತ್ರಿ', 'ರೀತಿಯಲ್ಲಿ', 'ವಕೀಲರನ್ನು', 'ವರೆಗೆ', 'ವಸಂತಕಾಲದಲ್ಲಿ', 'ವಹಿಸಿಕೊಂಡಿದನು', 'ವಾಪಸ್ಸು', 'ವಿದೇಶಕ್ಕೆ', 'ವೇಳೆ', 'ವೈನು', 'ಶುಭಾಕಾಂಕ್ಷೆಗಳನ್ನು', 'ಸಂಗ್ರಾಲಯಕ್ಕೆ', 'ಸಂಚಾರ', 'ಸಂತೋಷ', 'ಸಂಸ್ಥೆಯ', 'ಸತ್ಯವನ್ನು', 'ಸಮ್ಮತಿಸುತ್ತಾನೆ', 'ಸರಾಯಿ', 'ಸರಿಯಾಗಿ', 'ಸಲ', 'ಸಹಾಯ', 'ಸಾಕಷ್ಟು', 'ಸಾಕಿದಳು', 'ಸಾಧ್ಯ?', 'ಸಾಧ್ಯತೆ', 'ಸಾಮಾನ್ಯವಾಗಿ', 'ಸಿಕ್ಕಿತು', 'ಸಿಕ್ಕಿತು?', 'ಸಿಕ್ಕಿತ್ತೋ', 'ಸಿಕ್ಕಿಹಾಕಿಕೊಳ್ಳಿದ್ದೆವಿ', 'ಸುಚಿಸುತ್ತೀರಾ?', 'ಸುಧಾರಿಸುತ್ತಿದ್ದೇನೆ', 'ಸೂಚಿಸುತ್ತೇನೆ', 'ಸೊಚಿಸಬಹುದು', 'ಸೌಂದರ್ಯವಾಗಿದ್ದಾಳೆ', 'ಸ್ತಂಭನ', 'ಸ್ಥಳಾಂತರಿಸುತ್ತಾರೆ', 'ಸ್ವಲ್ಪ', 'ಸ್ವಾಗತಿಸೋಣ', 'ಹಂಗೇ', 'ಹಂದಿಮನೆ', 'ಹಣವಿದೆ', 'ಹದಿವಯಸ್ಸಿನಲ್ಲಿ', 'ಹಲವರು', 'ಹಾಗು', 'ಹುಡುಕಾಟವನ್ನು', 'ಹುಷಾರು', 'ಹೆಚ್ಚು', 'ಹೆದರುತ್ತವೆ', 'ಹೇಳಬೇಕೆಂದರೆ,', 'ಹೇಗೆ', 'ಹೇಳಕ್ಕೆ', 'ಹೇಳಬೇಕು', 'ಹೇಳಲಿಲ್ಲವೆಂದರೆ', 'ಹೇಳಲೇಬೇಕು', 'ಹೇಳಲ್ಲ', 'ಹೇಳಿ', 'ಹೇಳಿಕೊಟ್ಟನೆಂದು', 'ಹೇಳಿದನು', 'ಹೇಳುತ್ತೀರಾ?', 'ಹೊತ್ತು', 'ಹೊರಗೆ', 'ಹೊರಡುತ್ತಾ', 'ಹೊಸ', 'ಹೋಗಬಹುದಾ?', 'ಹೋಗಲು', 'ಹೋಗಲೇ', 'ಹೋಗಿ', 'ಹೋಗಿಲ್ಲ', 'ಹೋಗು', 'ಹೋಗುತ್ತಾನೆ', 'ಹೋಗುತ್ತಾನೆ?', 'ಹೋಗುವುದಕ್ಕೆ', 'ಹೋದ', 'ಹೋದ್ವಿ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaOrFQOgWBGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X,y = data['eng'], data['kan']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oXP8mjNWCne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOtdQcz4Y7WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 8):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_len_src),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_len_tar),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_len_tar, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8aupZ4pQigN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 25\n",
        "\n",
        "encoder_inputs = Input(shape = (None,))\n",
        "encoder_embedding = Embedding(num_encoder_tokens, latent_dim, mask_zero= True)(encoder_inputs)\n",
        "enc_lstm = LSTM(latent_dim, return_state= True)\n",
        "output , h , c = enc_lstm(encoder_embedding)\n",
        "\n",
        "encoder_states = [h,c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_emb = Embedding(num_decoder_tokens, latent_dim, mask_zero= True)\n",
        "dec_emb = decoder_emb(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences= True, return_state= True)\n",
        "decoder_output, _, _ = decoder_lstm(dec_emb,initial_state = encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiI33Q70V00M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 8\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BBTzwQYYF8I",
        "colab_type": "code",
        "outputId": "5d253ab4-156a-4e02-d662-ad679bfbe1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_samples)\n",
        "print(val_samples)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "134\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYAfmoiLWNHY",
        "colab_type": "code",
        "outputId": "68549771-2b79-4f01-a69b-0d197b1e5d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples/batch_size)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "16/16 [==============================] - 4s 220ms/step - loss: 3.1748 - acc: 0.1584 - val_loss: 2.8586 - val_acc: 0.1724\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 3.0247 - acc: 0.1780 - val_loss: 2.7483 - val_acc: 0.1724\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 2.8866 - acc: 0.1752 - val_loss: 2.6912 - val_acc: 0.1724\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.7504 - acc: 0.1767 - val_loss: 2.6928 - val_acc: 0.1724\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.7125 - acc: 0.1755 - val_loss: 2.7172 - val_acc: 0.1724\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 1s 67ms/step - loss: 2.6633 - acc: 0.1772 - val_loss: 2.7510 - val_acc: 0.1724\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.6776 - acc: 0.1752 - val_loss: 2.7895 - val_acc: 0.1724\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.6674 - acc: 0.1755 - val_loss: 2.8227 - val_acc: 0.1724\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.6438 - acc: 0.1757 - val_loss: 2.8623 - val_acc: 0.1724\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.6036 - acc: 0.1777 - val_loss: 2.9001 - val_acc: 0.1724\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.6401 - acc: 0.1748 - val_loss: 2.9244 - val_acc: 0.1724\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.5571 - acc: 0.1792 - val_loss: 2.9594 - val_acc: 0.1724\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.5963 - acc: 0.1762 - val_loss: 2.9726 - val_acc: 0.1724\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.5626 - acc: 0.1772 - val_loss: 2.9866 - val_acc: 0.1724\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.5689 - acc: 0.1760 - val_loss: 2.9971 - val_acc: 0.1724\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.5492 - acc: 0.1757 - val_loss: 3.0219 - val_acc: 0.1724\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.5426 - acc: 0.1752 - val_loss: 3.0457 - val_acc: 0.1724\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.5513 - acc: 0.1763 - val_loss: 3.0370 - val_acc: 0.1724\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.4662 - acc: 0.1780 - val_loss: 3.0739 - val_acc: 0.1724\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.5023 - acc: 0.1752 - val_loss: 3.0643 - val_acc: 0.1724\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.4604 - acc: 0.1767 - val_loss: 3.0827 - val_acc: 0.1724\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.4571 - acc: 0.1755 - val_loss: 3.0805 - val_acc: 0.1724\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 2.4217 - acc: 0.1772 - val_loss: 3.1329 - val_acc: 0.1724\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.4347 - acc: 0.1752 - val_loss: 3.1319 - val_acc: 0.1724\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 2.4247 - acc: 0.1755 - val_loss: 3.1378 - val_acc: 0.1724\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.3983 - acc: 0.1771 - val_loss: 3.1633 - val_acc: 0.1724\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.3551 - acc: 0.1791 - val_loss: 3.1559 - val_acc: 0.1724\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.3894 - acc: 0.1761 - val_loss: 3.2056 - val_acc: 0.1839\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.3075 - acc: 0.1821 - val_loss: 3.1984 - val_acc: 0.1839\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.3476 - acc: 0.1790 - val_loss: 3.1884 - val_acc: 0.1839\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.3186 - acc: 0.1857 - val_loss: 3.2503 - val_acc: 0.1839\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.3306 - acc: 0.1913 - val_loss: 3.2715 - val_acc: 0.1839\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.3159 - acc: 0.1953 - val_loss: 3.2935 - val_acc: 0.1839\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.3139 - acc: 0.1975 - val_loss: 3.2968 - val_acc: 0.1839\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.3243 - acc: 0.2025 - val_loss: 3.3843 - val_acc: 0.1839\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.2473 - acc: 0.2048 - val_loss: 3.3501 - val_acc: 0.1839\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.2909 - acc: 0.2017 - val_loss: 3.3126 - val_acc: 0.1839\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.2531 - acc: 0.2006 - val_loss: 3.3371 - val_acc: 0.1839\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.2470 - acc: 0.2019 - val_loss: 3.3651 - val_acc: 0.1839\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.2197 - acc: 0.2039 - val_loss: 3.3595 - val_acc: 0.1724\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.2328 - acc: 0.2045 - val_loss: 3.3839 - val_acc: 0.1724\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.2247 - acc: 0.2019 - val_loss: 3.3507 - val_acc: 0.1724\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 2.1971 - acc: 0.2064 - val_loss: 3.4457 - val_acc: 0.1724\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.1590 - acc: 0.2102 - val_loss: 3.4405 - val_acc: 0.1724\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.1908 - acc: 0.2039 - val_loss: 3.4284 - val_acc: 0.1839\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.1107 - acc: 0.2162 - val_loss: 3.4518 - val_acc: 0.1954\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.1511 - acc: 0.2140 - val_loss: 3.4669 - val_acc: 0.1954\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.1217 - acc: 0.2180 - val_loss: 3.4802 - val_acc: 0.1954\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 2.1319 - acc: 0.2151 - val_loss: 3.4620 - val_acc: 0.1954\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 2.1163 - acc: 0.2148 - val_loss: 3.4583 - val_acc: 0.1954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f333f4cbeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s8qFyykW_EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('drive/My Drive/Colab Notebooks/Machine_Translation/kan-eng/mt_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vi6YnuOboSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('drive/My Drive/Colab Notebooks/Machine_Translation/kan-eng/mt_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0sI58TpfG80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2= decoder_emb(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IRjYyA97E6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = target_token_index['<SOS>']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "        if (sampled_char == '<EOS>' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPFqVo6l7lff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_7Px0Qe73oz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0e8f25f1-0b3c-4abd-f511-f25b1272a151"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Kannada Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Kannada Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: he had an accident at work\n",
            "Actual Kannada Translation: ಕೆಲಸದಲ್ಲಿ ಅವನಿಗೆ ಅಪಘಾತವಾಯಿತು <\n",
            "Predicted Kannada Translation:  ನಾನು ಬೇಕಿತ್ತೋ ಅಂತ ಹಂದಿಮನೆ <\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgPUyD17_o_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}